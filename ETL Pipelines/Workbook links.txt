## ETL using shell scripts
https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Bash%20Scripting/ETL%20using%20shell%20scripting/ETL%20using%20shell%20scripting.md.html

## Getting started with Apache Airflow
https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Getting%20Started%20with%20Apache%20Airflow/Getting%20started%20with%20Apache%20Airflow.md.html

## Create a DAG for Apache Airflow
https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/Build%20a%20DAG%20using%20Airflow.md.html

## Monitoring a DAG
https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTS1EQjAyNTBFTi1Ta2lsbHNOZXR3b3JrL2xhYnMvQXBhY2hlJTIwQWlyZmxvdy9Nb25pdG9yaW5nJTIwYSUyMERBRy9IYW5kcy1vbl9MYWItX01vbml0b3JpbmdfYV9EQUcubWQiLCJ0b29sX3R5cGUiOiJ0aGVpYWRvY2tlciIsImFkbWluIjpmYWxzZSwiaWF0IjoxNjczNDI3OTA2fQ.nZHaWOfRyPTfD_2h16NMaSOqJIQ7O5Eoe2PBBmmJPEQ

## Working with streaming data on Kafka
https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTS1EQjAyNTBFTi1Ta2lsbHNOZXR3b3JrL2xhYnMvU3RyZWFtaW5nL1N0cmVhbWluZyUyMGRhdGElMjB3aXRoJTIwa2Fma2EubWQiLCJ0b29sX3R5cGUiOiJ0aGVpYWRvY2tlciIsImFkbWluIjpmYWxzZSwiaWF0IjoxNjcyMTIzMzcwfQ.yUJM8xGyQe3ksZoZtL-2rdpZwpocs5eJHRd5x3_V2VE

## Kafka Message key and offset
https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTS1EQjAyNTBFTi1Ta2lsbHNOZXR3b3JrL2xhYnMvU3RyZWFtaW5nL2xhYi1vcHRpb25hbC1rYWZrYS1tc2drZXlfb2Zmc2V0Lm1kIiwidG9vbF90eXBlIjoidGhlaWFkb2NrZXIiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTY3MjEyMzcwN30.HJsRrze4w2bRyjCV2p14IxikrQGq4ptWFqdCl8O5gJ8

## Creating ETL Data Pipelines using Apache Airflow
https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL0lCTS1EQjAyNTBFTi1Ta2lsbHNOZXR3b3JrL2xhYnMvRmluYWwlMjBBc3NpZ25tZW50L0VUTF9QZWVyX1Jldmlld19Bc3NpZ25tZW50Lm1kIiwidG9vbF90eXBlIjoidGhlaWFkb2NrZXIiLCJhZG1pbiI6ZmFsc2UsImlhdCI6MTY3MjEyNDIyMn0.TyI9Sn7aIcAowhKVxW7qPJoOvKDRjxTVYnld-gYyggk






# import the libraries

from datetime import timedelta
# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG
# Operators; we need this to write tasks!
from airflow.operators.bash_operator import BashOperator
# This makes scheduling easy
from airflow.utils.dates import days_ago


# TASK 1.1
#defining DAG arguments

# You can override them on a per-task basis during operator initialization
dag_args = {
    'owner': 'Mimansa',
    'start_date': days_ago(0),
    'email': ['mimansa@somemail.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# TASK 1.2
# define the DAG
dag = DAG(
    DAGid = 'ETL_toll_data',
    default_args = dag_args,
    description='Unzip folder and store in form of csv files',
    schedule_interval=timedelta(days=1),
)

# TASK 1.3
extract = BashOperator(
    task_id='first task',
    bash_command='',
    dag=dag,
)


